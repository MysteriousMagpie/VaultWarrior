Metadata-Version: 2.4
Name: vault-ai-cli
Version: 0.1.0
Summary: Vault-aware RAG planning CLI for Obsidian vaults
Author-email: Your Name <you@example.com>
License: MIT License
        
        Copyright (c) 2025 MysteriousMagpie
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Intended Audience :: Developers
Classifier: Environment :: Console
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typer[all]>=0.12.0
Requires-Dist: rich>=13.7.0
Requires-Dist: pyyaml>=6.0.1
Requires-Dist: sentence-transformers>=2.7.0
Requires-Dist: faiss-cpu>=1.8.0
Requires-Dist: openai>=1.30.0
Requires-Dist: watchfiles>=0.21.0
Requires-Dist: gitpython>=3.1.43
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Provides-Extra: web
Requires-Dist: fastapi>=0.110.0; extra == "web"
Requires-Dist: uvicorn[standard]>=0.29.0; extra == "web"
Dynamic: license-file

# AI Vault Planning CLI (early scaffold)

Opinionated, vault‑aware Retrieval Augmented Generation (RAG) + planning CLI named `ai` for Obsidian‑style markdown vaults. Start totally from scratch and turn a plain note folder into an indexed, queryable, threaded planning surface.

## Features (implemented so far)
- `ai init <vault_path>` creates `_ai/config.yaml`, thread, and daily directories.
- `ai index <vault_path>` builds FAISS embeddings index (sentence-transformers) over markdown files.
- `ai thread new <slug>` creates a new thread file with frontmatter.
- `ai ask "question"` performs retrieval and prints citation list.
- `ai chat <slug> "message" --write` streams an OpenAI answer (requires API key) and appends to thread.
- `ai capture "text" --write` appends quick capture to the daily note.
- `ai plan <slug> --weekly --write` produces a checkpoint summary (placeholder prompt) and appends.
- `ai doctor` basic sanity checks (presence of config and index).

## Not Yet Implemented
- Watch / incremental indexing (`ai index --watch`, `ai daemon`).
- Advanced filters (frontmatter tags, globs) beyond placeholders.
- Robust error handling & rate limiting.
- Model backend abstraction beyond OpenAI placeholder.

## Install (dev)
```bash
pip install -e .[dev]
```

Or create / activate a virtualenv first:
```bash
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -e .[dev]
```

## Quickstart (from absolute zero)

```bash
# 1. (Optional) create a test vault
mkdir -p ~/MyVault && cd ~/MyVault
echo "# Project X\n\nInitial notes." > ProjectX.md

# 2. Initialize AI scaffolding
ai init .

# 3. Build the embeddings index (re-run after note edits)
ai index .

# 4. Start a planning / discussion thread
ai thread new kickoff --vault-path . --seed "Kickoff notes: clarify scope"

# 5. Ask an ad-hoc question (retrieval only)
ai ask "What did I note about scope?" --vault-path .

# 6. (If chatting / planning) set API key
export OPENAI_API_KEY=sk-...   # required for chat / plan

# 7. Continue threaded chat and persist answer
ai chat kickoff "Summarize current scope and next 2h task" --vault-path . --write

# 8. Add a quick capture into today's daily note
ai capture "Idea: Add design sketch for data pipeline" --vault-path . --write

# 9. Weekly planning checkpoint (placeholder prompt right now)
ai plan kickoff --vault-path . --weekly --write

# 10. Sanity check the setup
ai doctor --vault-path .
```

## Mental Model / Concepts

| Concept | What It Is | Where It Lives |
|---------|------------|----------------|
| Vault | Your markdown knowledge base | Any folder you point `ai` at |
| Config | Minimal YAML controlling models/index path | `_ai/config.yaml` |
| Index | FAISS vector store built from chunks of notes | `_ai/index/` |
| Thread | Persistent conversation/planning log with frontmatter | `_ai/threads/<slug>.md` |
| Daily Note | Date-stamped capture file | `_ai/daily/YYYY-MM-DD.md` |
| Capture | Fast append of a snippet to today's daily | Command `ai capture` |
| Ask | One-off retrieval + citations (no write) | `ai ask` |
| Chat | Retrieval + LLM answer inside a thread | `ai chat <slug>` |
| Plan | Periodic summarization / checkpoint (LLM) | `ai plan <slug>` |

## Typical Flow

1. Add / edit notes normally in your editor.
2. Run `ai index` when you've materially changed notes (incremental watch mode is future work).
3. Use `ai ask` for quick factual retrieval with citations.
4. Promote ongoing work to a thread: `ai thread new feature-x` then `ai chat feature-x ... --write`.
5. Periodically run `ai plan feature-x --weekly --write` to generate a checkpoint summary.
6. Capture stray ideas quickly with `ai capture` so they are searchable after next index build.

## Command Reference (current subset)

```text
ai init <vault_path>
	Scaffold `_ai/` directory (config, threads, daily). Idempotent.

ai index <vault_path>
	Parse markdown, chunk, embed, build / replace FAISS index.

ai thread new <slug> [--vault-path PATH] [--seed TEXT]
	Create a new thread file with optional seed content.

ai ask "question" [--vault-path PATH]
	Retrieve top chunks + print answer provenance (citations list). No write side-effects.

ai chat <slug> "message" [--vault-path PATH] [--write]
	Retrieval + streaming answer in context of thread; `--write` appends assistant message.

ai capture "text" [--vault-path PATH] [--write]
	Append snippet to today's daily note (with timestamp). `--write` persists (planned default later).

ai plan <slug> [--weekly] [--vault-path PATH] [--write]
	Run planning / summarization prompt variant; cadence flag influences prompt flavor.

ai doctor [--vault-path PATH]
	Lightweight checks (config exists, index present, etc.).

ai enrich [--vault-path PATH] [--apply] [--no-add-tags]
	Dry-run (default) or apply normalization of frontmatter (title, created, inferred tags).
```

### Environment Variables

| Variable | Required | Purpose |
|----------|----------|---------|
| `OPENAI_API_KEY` | For chat/plan | Auth for LLM calls (only OpenAI placeholder currently). |
| `AI_VAULT_MODEL` | Optional | Override default model name (if support added). |

### File Layout After Init

```
_ai/
  config.yaml
  index/                # FAISS files
  threads/
	kickoff.md
  daily/
	2025-08-19.md
```

### Thread File Sketch

```markdown
---
slug: kickoff
created: 2025-08-19T12:34:56Z
---
# kickoff

> seed: Kickoff notes: clarify scope

## 2025-08-19 12:40
user: Summarize current scope and next 2h task
assistant: (LLM answer...) 
```

## Updating / Re-indexing Strategy

Until watch mode exists, batch updates: make note edits, then run `ai index`. For large vaults, consider excluding heavy folders (images, attachments) at the file system or future ignore patterns (planned).

## Git Hygiene

You may prefer to ignore raw FAISS artifacts:
```
_ai/index/*
!_ai/index/.gitkeep
```
Keep config, threads, daily notes versioned for change history.

## Troubleshooting

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| `ai` command not found | Editable install not done / wrong venv | Re-run install inside active venv: `pip install -e .[dev]` |
| Chat fails with auth error | Missing `OPENAI_API_KEY` | `export OPENAI_API_KEY=sk-...` |
| Retrieval returns nothing | Index not built or outdated | Run `ai index <vault>` again |
| Thread file not updating | Forgot `--write` | Add `--write` flag |
| Slow indexing | Very large vault | Future: incremental watch; meanwhile prune or split vault |

## Extending (High-Level)

Planned extension seams:
* Backend LLM provider abstraction (currently single OpenAI placeholder).
* Incremental indexer / watcher daemon.
* Advanced retrieval filters (frontmatter tags, path globs, temporal recency weighting).
* Automatic thread summarization windowing.
* Git auto-commit policy for captures and thread deltas.

## Minimal Programmatic Use (Illustrative Only)

Once modules stabilize you might import internal APIs (names subject to change):
```python
from ai.core.index import build_index
from ai.core.retrieve import retrieve

index = build_index(vault_path="/path/to/Vault")
results = retrieve(index, query="open tasks in project x")
for r in results:
	print(r.score, r.path)
```
API surface is not locked yet; expect refactors.

## Security / Privacy Notes

All vault parsing + embedding happens locally; only the prompt content and selected retrieved chunks are sent to the LLM provider (OpenAI) when you invoke chat/plan. Sensitive content redaction helpers are roadmap items (tests reference forthcoming redaction logic).

## Performance Tips

* Favor shorter headings / sections; chunker likely splits by heading and length threshold.
* Rebuild index during natural breaks; heavy continuous editing can wait for a single batch.
* Keep high-churn scratch notes small to avoid constant large-chunk invalidation later (post‑incremental feature).

## Contributing

Run tests before proposing changes:
```bash
pytest -q
```
Style / lint tooling will be added as project matures.

---

If you need deeper details see `Init.md` or ask for a specific area (indexing pipeline, retrieval ranking, planning prompt scaffolds, etc.).

## Usage Example
```bash
ai init /path/to/Vault
ai index /path/to/Vault
ai thread new demo --vault-path /path/to/Vault --seed "Kickoff notes"
ai chat demo "What is my next 2-hour task on project X?" --vault-path /path/to/Vault --write
```

Set your environment variable for OpenAI before chat / plan:
```bash
export OPENAI_API_KEY=sk-...
```

## Tests
```bash
pytest -q
```

## Roadmap
See `Init.md` for full specification; remaining work includes daemon, advanced filtering, citation enrichment, thread summarization windowing, write policy enforcement, and git auto-commit improvements.
